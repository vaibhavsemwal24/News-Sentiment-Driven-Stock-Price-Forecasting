{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e491920",
   "metadata": {},
   "source": [
    "# scraping news from Google news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a292e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import datetime\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the keywords and the number of years to scrape\n",
    "keywords = [\"amazon\", \"aws\"]\n",
    "num_years = 5\n",
    "\n",
    "# Get the current date and the date `num_years` ago\n",
    "now = datetime.datetime.now()\n",
    "start_date = now.replace(year=now.year - num_years)\n",
    "\n",
    "# Create empty lists to store the news headlines, dates, and URLs\n",
    "headlines = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Loop through the dates and scrape the news for each date\n",
    "current_date = start_date\n",
    "while current_date <= now:\n",
    "    # Format the date to match the RSS feed's date format\n",
    "    date_str = current_date.strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    # Loop through the keywords and scrape the news for each keyword\n",
    "    for keyword in keywords:\n",
    "        # Build the URL for the RSS feed\n",
    "        query = f\"{keyword} when:{date_str}\"\n",
    "        query = urllib.parse.quote_plus(query)\n",
    "        url = f\"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        \n",
    "        # Parse the RSS feed and extract the news headlines, dates, and URLs\n",
    "        feed = feedparser.parse(url)\n",
    "        for entry in feed.entries:\n",
    "            headlines.append(entry.title)\n",
    "            dates.append(current_date.date())\n",
    "            urls.append(entry.link)\n",
    "    \n",
    "    # Move to the next date\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "\n",
    "# Create a dataframe with the news headlines, dates, and URLs\n",
    "df = pd.DataFrame({\"date\": dates, \"headline\": headlines, \"url\": urls})\n",
    "\n",
    "# saving the dataframe\n",
    "df.to_csv('google_news.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5238fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2017</td>\n",
       "      <td>Forensic Amazon Analysis: A Value Equation App...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiW2h0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-2017</td>\n",
       "      <td>I bought Bitcoin from PayPal. Here's what happ...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiWmh0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-2017</td>\n",
       "      <td>At least 9 dead in Ugandan New Year firework c...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiTWh0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-2017</td>\n",
       "      <td>Tolino Epos 2 e-Reader Review - Good e-Reader</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiQmh0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-2017</td>\n",
       "      <td>Which topics would you like to discuss with us...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiXWh0d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                           headline  \\\n",
       "0  01-01-2017  Forensic Amazon Analysis: A Value Equation App...   \n",
       "1  01-01-2017  I bought Bitcoin from PayPal. Here's what happ...   \n",
       "2  01-01-2017  At least 9 dead in Ugandan New Year firework c...   \n",
       "3  01-01-2017      Tolino Epos 2 e-Reader Review - Good e-Reader   \n",
       "4  01-01-2017  Which topics would you like to discuss with us...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://news.google.com/rss/articles/CBMiW2h0d...  \n",
       "1  https://news.google.com/rss/articles/CBMiWmh0d...  \n",
       "2  https://news.google.com/rss/articles/CBMiTWh0d...  \n",
       "3  https://news.google.com/rss/articles/CBMiQmh0d...  \n",
       "4  https://news.google.com/rss/articles/CBMiXWh0d...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "google_news=pd.read_csv('google_news.csv')\n",
    "google_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898cd9c",
   "metadata": {},
   "source": [
    "# scraping news from nytimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "api_key = \"ygAc9UuGvItoIu2SLRkdEVwedLAuSoca\" #api provided by the nytimes\n",
    "\n",
    "# Prompt for start and end dates\n",
    "start_date_str = input(\"Enter start date (YYYY-MM-DD): \")\n",
    "end_date_str = input(\"Enter end date (YYYY-MM-DD): \")\n",
    "start_date = dt.datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "end_date = dt.datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "#asking for the keywords that we want to search\n",
    "company_names = []\n",
    "while True:\n",
    "    company_name = input(\"Enter search query (or press enter to stop): \")\n",
    "    if company_name == '':\n",
    "        break\n",
    "    company_names.append(company_name)\n",
    "\n",
    "# Loop through each keyword and searching for articles\n",
    "for company_name in company_names:\n",
    "    print(f'Searching for \"{company_name}\" on New York Times...')\n",
    "\n",
    "    # Specifying the number of pages you want to scrape\n",
    "    num_pages = 1000  # Replace with the desired number of pages\n",
    "\n",
    "    # Building the API request URL and retrieve the data for each page\n",
    "    result = []\n",
    "    for page in range(num_pages):\n",
    "        url = f\"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key={api_key}&q={company_name}&begin_date={start_date}&end_date={end_date}&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error occurred: {response.text}\")\n",
    "            break\n",
    "        data = response.json()\n",
    "\n",
    "        # Extracting the relevant data from the response for the current page\n",
    "        articles = data[\"response\"][\"docs\"]\n",
    "        for article in articles:\n",
    "            article_dict = {}\n",
    "            article_dict[\"title\"] = article[\"headline\"][\"main\"]\n",
    "            article_dict[\"date\"] = article[\"pub_date\"]\n",
    "            article_dict[\"url\"] = article[\"web_url\"]\n",
    "            article_dict[\"content\"] = article[\"abstract\"] if \"abstract\" in article else \"\"\n",
    "            result.append(article_dict)\n",
    "\n",
    "    # Converting the result to a pandas DataFrame and print it\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(\"nytimes_news\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d74c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 11:24:45+00:00</td>\n",
       "      <td>Unboxed Extra: I.B.M. and Americaâ€™s Job Challenge</td>\n",
       "      <td>https://bits.blogs.nytimes.com/2012/01/01/unbo...</td>\n",
       "      <td>I.B.M. is a corporate pioneer in globalization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-02 23:58:32+00:00</td>\n",
       "      <td>On Wall Street, Renewed Optimism for Deal-Making</td>\n",
       "      <td>https://dealbook.nytimes.com/2012/01/02/on-wal...</td>\n",
       "      <td>According to a recent study by Ernst &amp; Young, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-06 18:39:50+00:00</td>\n",
       "      <td>A Historical Cycle Bodes Ill for the Markets</td>\n",
       "      <td>https://www.nytimes.com/2012/01/07/business/ec...</td>\n",
       "      <td>In what appears to be a recurring 15-year cycl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-09 12:00:31+00:00</td>\n",
       "      <td>This Week in Small Business: For the Win!</td>\n",
       "      <td>https://boss.blogs.nytimes.com/2012/01/09/this...</td>\n",
       "      <td>Plus: Do your employees offer dissenting viewp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-10 02:19:27+00:00</td>\n",
       "      <td>UniCreditâ€™s Weak Share Offering a Poor Omen in...</td>\n",
       "      <td>https://dealbook.nytimes.com/2012/01/09/unicre...</td>\n",
       "      <td>Even a steep discount drew tepid interest in U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2012-01-01 11:24:45+00:00   \n",
       "1  2012-01-02 23:58:32+00:00   \n",
       "2  2012-01-06 18:39:50+00:00   \n",
       "3  2012-01-09 12:00:31+00:00   \n",
       "4  2012-01-10 02:19:27+00:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Unboxed Extra: I.B.M. and Americaâ€™s Job Challenge   \n",
       "1   On Wall Street, Renewed Optimism for Deal-Making   \n",
       "2       A Historical Cycle Bodes Ill for the Markets   \n",
       "3          This Week in Small Business: For the Win!   \n",
       "4  UniCreditâ€™s Weak Share Offering a Poor Omen in...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://bits.blogs.nytimes.com/2012/01/01/unbo...   \n",
       "1  https://dealbook.nytimes.com/2012/01/02/on-wal...   \n",
       "2  https://www.nytimes.com/2012/01/07/business/ec...   \n",
       "3  https://boss.blogs.nytimes.com/2012/01/09/this...   \n",
       "4  https://dealbook.nytimes.com/2012/01/09/unicre...   \n",
       "\n",
       "                                             content  \n",
       "0  I.B.M. is a corporate pioneer in globalization...  \n",
       "1  According to a recent study by Ernst & Young, ...  \n",
       "2  In what appears to be a recurring 15-year cycl...  \n",
       "3  Plus: Do your employees offer dissenting viewp...  \n",
       "4  Even a steep discount drew tepid interest in U...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nytimes_news=pd.read_csv('nytimes_news.csv')\n",
    "nytimes_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47e701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf24b82",
   "metadata": {},
   "source": [
    "# merging both news datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d64fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the first CSV file into a Pandas DataFrame\n",
    "df1 = pd.read_csv('merged_file.csv')\n",
    "df1['date'] = pd.to_datetime(df1['date'])  # Convert \"date\" column to datetime\n",
    "df1.set_index('date', inplace=True)  # Set \"date\" as the index\n",
    "\n",
    "# Loading the second CSV file into a Pandas DataFrame\n",
    "df2 = pd.read_csv('2022.csv')\n",
    "df2['date'] = pd.to_datetime(df2['date'])  # Convert \"date\" column to datetime\n",
    "df2.set_index('date', inplace=True)  # Set \"date\" as the index\n",
    "\n",
    "# Concatenating the two DataFrames along the rows (axis=0)\n",
    "merged_df = pd.concat([df1, df2])\n",
    "\n",
    "# Sorting the merged DataFrame by the index (date)\n",
    "merged_df.sort_index(inplace=True)\n",
    "\n",
    "# Saving the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('google and nytimes news.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65887ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2012</td>\n",
       "      <td>Unboxed Extra: I.B.M. and AmericaÃ¢â‚¬â„¢s Job Chal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2012</td>\n",
       "      <td>On Wall Street, Renewed Optimism for Deal-Making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06-01-2012</td>\n",
       "      <td>A Historical Cycle Bodes Ill for the Markets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09-01-2012</td>\n",
       "      <td>This Week in Small Business: For the Win!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-01-2012</td>\n",
       "      <td>UniCreditÃ¢â‚¬â„¢s Weak Share Offering a Poor Omen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                              title\n",
       "0  01-01-2012  Unboxed Extra: I.B.M. and AmericaÃ¢â‚¬â„¢s Job Chal...\n",
       "1  02-01-2012   On Wall Street, Renewed Optimism for Deal-Making\n",
       "2  06-01-2012       A Historical Cycle Bodes Ill for the Markets\n",
       "3  09-01-2012          This Week in Small Business: For the Win!\n",
       "4  10-01-2012  UniCreditÃ¢â‚¬â„¢s Weak Share Offering a Poor Omen ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_and_nytimes_news=pd.read_csv('google and nytimes news.csv')\n",
    "google_and_nytimes_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83a5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a28a0d",
   "metadata": {},
   "source": [
    "# performing sentiment analysis on the news data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea746dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"2022.csv\")\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to perform sentiment analysis on the title column\n",
    "def analyze_sentiment(title):\n",
    "    # Performing sentiment analysis on the title\n",
    "    sentiment = analyzer.polarity_scores(title)\n",
    "    compound = sentiment[\"compound\"]\n",
    "    positive = sentiment[\"pos\"]\n",
    "    negative = sentiment[\"neg\"]\n",
    "    neutral = sentiment[\"neu\"]\n",
    "\n",
    "    # Performing subjectivity analysis on the title\n",
    "    subjectivity = TextBlob(title).sentiment.subjectivity\n",
    "\n",
    "    return pd.Series({\"compound\": compound, \"subjectivity\": subjectivity, \"positive\": positive, \"negative\": negative, \"neutral\": neutral})\n",
    "\n",
    "# Apply the sentiment analysis function to the title column\n",
    "sentiment_df = df[\"title\"].apply(analyze_sentiment)\n",
    "\n",
    "# Concatenate the original dataframe and the sentiment dataframe\n",
    "df = pd.concat([df, sentiment_df], axis=1)\n",
    "\n",
    "# Print the updated dataframe\n",
    "df.to_cav('news sentiment data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0598194f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>title</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2012</td>\n",
       "      <td>Unboxed Extra: I.B.M. and AmericaÃ¢â‚¬â„¢s Job Chal...</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2012</td>\n",
       "      <td>On Wall Street, Renewed Optimism for Deal-Making</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06-01-2012</td>\n",
       "      <td>A Historical Cycle Bodes Ill for the Markets</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09-01-2012</td>\n",
       "      <td>This Week in Small Business: For the Win!</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-01-2012</td>\n",
       "      <td>UniCreditÃ¢â‚¬â„¢s Weak Share Offering a Poor Omen ...</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.6125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                              title  compound  \\\n",
       "0  01-01-2012  Unboxed Extra: I.B.M. and AmericaÃ¢â‚¬â„¢s Job Chal...    0.9052   \n",
       "1  02-01-2012   On Wall Street, Renewed Optimism for Deal-Making    0.0000   \n",
       "2  06-01-2012       A Historical Cycle Bodes Ill for the Markets    0.0000   \n",
       "3  09-01-2012          This Week in Small Business: For the Win!    0.0000   \n",
       "4  10-01-2012  UniCreditÃ¢â‚¬â„¢s Weak Share Offering a Poor Omen ...    0.1779   \n",
       "\n",
       "   positive  negative  neutral  subjectivity  \n",
       "0     0.172     0.000    0.828        0.1000  \n",
       "1     0.000     0.000    1.000        0.0000  \n",
       "2     0.000     0.000    1.000        0.5000  \n",
       "3     0.000     0.000    1.000        0.4000  \n",
       "4     0.110     0.084    0.806        0.6125  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sentiment_data= pd.read_csv('news sentiment data.csv')\n",
    "news_sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66549407",
   "metadata": {},
   "source": [
    "# getting the stock data from yahoofinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Defining the ticker symbol and date range\n",
    "ticker = \"AMZN\"\n",
    "start_date = \"2012-01-31\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Geting the data\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Print the data\n",
    "data.head()\n",
    "data.to_csv(\"AMZN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc348da0",
   "metadata": {},
   "source": [
    "# merging the stock dataset and the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging two files\n",
    "import pandas as pd\n",
    "# Load the two CSV files into pandas dataframes\n",
    "df1 = pd.read_excel(\"C:/Users/vaibhav semwal/Desktop/merged1.xlsx\")\n",
    "df2 = pd.read_excel(\"C:/Users/vaibhav semwal/Desktop/NDX (2).xlsx\")\n",
    "\n",
    "# Converting the date column in both dataframes to a common format\n",
    "df1['Date'] = pd.to_datetime(df1['Date'], format='%d-%m-%Y')\n",
    "df2['Date'] = pd.to_datetime(df2['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# Merging the dataframes based on the 'date' column\n",
    "merged_df = pd.merge(df1, df2, on='Date', how='left')\n",
    "\n",
    "# Saving the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('sentiments and stock.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dced19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>title</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>A Historical Cycle Bodes Ill for the Markets</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>8.9035</td>\n",
       "      <td>9.2325</td>\n",
       "      <td>8.8750</td>\n",
       "      <td>9.1305</td>\n",
       "      <td>9.1305</td>\n",
       "      <td>140168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>This Week in Small Business: For the Win!</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>9.1380</td>\n",
       "      <td>9.2185</td>\n",
       "      <td>8.8500</td>\n",
       "      <td>8.9280</td>\n",
       "      <td>8.9280</td>\n",
       "      <td>101138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>UniCreditÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s Weak Share Offering a Poor ...</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>9.0550</td>\n",
       "      <td>9.1200</td>\n",
       "      <td>8.8550</td>\n",
       "      <td>8.9670</td>\n",
       "      <td>8.9670</td>\n",
       "      <td>79716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>Stanley Kwan, Hang Seng Index Creator, Dies at 86</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.9820</td>\n",
       "      <td>9.0385</td>\n",
       "      <td>8.9095</td>\n",
       "      <td>8.9450</td>\n",
       "      <td>8.9450</td>\n",
       "      <td>62054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-17</td>\n",
       "      <td>European Central Bankers Criticize Role of Rat...</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>9.0075</td>\n",
       "      <td>9.1650</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>9.0830</td>\n",
       "      <td>9.0830</td>\n",
       "      <td>112890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date                                              title  \\\n",
       "0           0  2012-01-06       A Historical Cycle Bodes Ill for the Markets   \n",
       "1           1  2012-01-09          This Week in Small Business: For the Win!   \n",
       "2           2  2012-01-10  UniCreditÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s Weak Share Offering a Poor ...   \n",
       "3           3  2012-01-11  Stanley Kwan, Hang Seng Index Creator, Dies at 86   \n",
       "4           4  2012-01-17  European Central Bankers Criticize Role of Rat...   \n",
       "\n",
       "   compound  positive  negative  neutral  subjectivity    Open    High  \\\n",
       "0    0.0000      0.00     0.000    1.000        0.5000  8.9035  9.2325   \n",
       "1    0.0000      0.00     0.000    1.000        0.4000  9.1380  9.2185   \n",
       "2    0.1779      0.11     0.084    0.806        0.6125  9.0550  9.1200   \n",
       "3    0.0000      0.00     0.000    1.000        0.0000  8.9820  9.0385   \n",
       "4   -0.1027      0.00     0.053    0.947        0.1250  9.0075  9.1650   \n",
       "\n",
       "      Low   Close  Adj Close     Volume  \n",
       "0  8.8750  9.1305     9.1305  140168000  \n",
       "1  8.8500  8.9280     8.9280  101138000  \n",
       "2  8.8550  8.9670     8.9670   79716000  \n",
       "3  8.9095  8.9450     8.9450   62054000  \n",
       "4  8.9255  9.0830     9.0830  112890000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_and_stock= pd.read_csv('sentiments and stock.csv')\n",
    "sentiments_and_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675da8b",
   "metadata": {},
   "source": [
    "# performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ff6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"final datasetcsv\")\n",
    "\n",
    "# Generate a report with statistics about each column\n",
    "report = pp.ProfileReport(df)\n",
    "\n",
    "# Print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728a4af",
   "metadata": {},
   "source": [
    "# Applying Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df =pd.read_csv(\"final dataset.csv\")\n",
    "\n",
    "# Replace infinite values with NaN and fill with column means\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[['compound', 'subjectivity', 'Open']]\n",
    "y_high = df['High']\n",
    "y_low = df['Low']\n",
    "X_train, X_test, y_high_train, y_high_test, y_low_train, y_low_test = train_test_split(X, y_high, y_low, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model to predict high column\n",
    "lr_high = LinearRegression()\n",
    "lr_high.fit(X_train, y_high_train)\n",
    "\n",
    "# Train a linear regression model to predict low column\n",
    "lr_low = LinearRegression()\n",
    "lr_low.fit(X_train, y_low_train)\n",
    "\n",
    "# Predict the high and low columns on the testing set\n",
    "y_high_pred = lr_high.predict(X_test)\n",
    "y_low_pred = lr_low.predict(X_test)\n",
    "\n",
    "# Calculate the root mean squared error for high and low predictions\n",
    "rmse_high = np.sqrt(mean_squared_error(y_high_test, y_high_pred))\n",
    "rmse_low = np.sqrt(mean_squared_error(y_low_test, y_low_pred))\n",
    "\n",
    "print(f\"RMSE for High column: {rmse_high}\")\n",
    "print(f\"RMSE for Low column: {rmse_low}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d75d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the input features for the new data\n",
    "new_data = pd.DataFrame({'compound': [-0.5251], 'subjectivity': [0.387777778], 'Open': [98.7]})\n",
    "\n",
    "# Predict the high and low columns for the new data using the trained models\n",
    "new_high_pred = lr_high.predict(new_data)\n",
    "new_low_pred = lr_low.predict(new_data)\n",
    "\n",
    "print(f\"Predicted High: {new_high_pred[0]}\")\n",
    "print(f\"Predicted Low: {new_low_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a9c32",
   "metadata": {},
   "source": [
    "# Applying LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6726da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Define SMAPE metric for evaluation\n",
    "def smape_kun(y_true, y_pred):\n",
    "    return np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))))\n",
    "\n",
    "# Load data from CSV file and set 'Date' column as index\n",
    "data = pd.read_csv('final dataset.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Select relevant columns and drop any rows with missing values\n",
    "exog_vars = ['subjectivity','compound', 'Open','High','Adj Close']\n",
    "data = data[exog_vars + ['Close']].dropna()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(data) * 0.7)\n",
    "train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
    "\n",
    "# Normalize data using min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Define function to create time series dataset\n",
    "def create_time_series_dataset(X, y, lookback):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - lookback):\n",
    "        Xs.append(X[i:(i+lookback)])\n",
    "        ys.append(y[i+lookback])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Define hyperparameters and create time series datasets\n",
    "lookback = 60\n",
    "batch_size = 32\n",
    "train_X, train_y = create_time_series_dataset(train_data_scaled, train_data_scaled[:, -1], lookback)\n",
    "test_X, test_y = create_time_series_dataset(test_data_scaled, test_data_scaled[:, -1], lookback)\n",
    "\n",
    "# Define LSTM model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(lookback, train_X.shape[-1])),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and fit model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(train_X, train_y, epochs=10, batch_size=batch_size, validation_data=(test_X, test_y))\n",
    "\n",
    "# Make predictions on test set and scale back to original values\n",
    "predictions = model.predict(test_X)\n",
    "predictions = scaler.inverse_transform(np.concatenate((test_X[:, -1, :-1], predictions), axis=1))[:, -1]\n",
    "\n",
    "# Calculate error metrics\n",
    "error = mean_squared_error(test_data['Close'][lookback:], predictions)\n",
    "print('Testing Mean Squared Error: %.3f' % error)\n",
    "\n",
    "error2 = smape_kun(test_data['Close'][lookback:].values, predictions)\n",
    "print('Symmetric mean absolute percentage error: %.3f' % error2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_data.index[lookback:], test_data['Close'][lookback:], label='Actual')\n",
    "plt.plot(test_data.index[lookback:], predictions, label='Predicted')\n",
    "plt.title('Actual vs Predicted Close Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d997007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680615f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfbd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fc0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7dce96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd448726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd1691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc542ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4e3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa00ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de900e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0286ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f3544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f2eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba339a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394131c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
